{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NLP - Useful NLP Libraries & Networks\n",
        "\n",
        "                         SUBMITTED BY: MD FAHAM NAUSHAD"
      ],
      "metadata": {
        "id": "9sRLjMJxi7iK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***************************************************\n",
        "##Question\n",
        "##***************************************************"
      ],
      "metadata": {
        "id": "7bN9dQwfjlHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance.\n",
        "\n",
        "- Answer:\n",
        "  \n",
        "  NLTK is a traditional NLP toolkit focused on teaching and research, offering granular linguistic functions like stemming, parsing, and corpora access. spaCy is designed for industrial use, optimized for fast processing and modern ML pipelines. NLTK provides flexibility but requires more manual steps, while spaCy offers pretrained pipelines and high-speed operations. NLTK is easier for beginners, whereas spaCy is preferred for production deployments due to performance.\n",
        "\n",
        "\n",
        "##Question 2: What is TextBlob and how does it simplify common NLP tasks like sentiment analysis and translation?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "  TextBlob is a Python library built on top of NLTK that simplifies everyday NLP operations using a clean and high-level API. It allows sentiment analysis, noun phrase extraction, translation, tagging, and classification with only a few lines of code. It is beginner-friendly and abstracts complex NLP logic behind simple function calls. Because of this, TextBlob is useful for quick prototypes and educational applications.\n",
        "\n",
        "##Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
        "\n",
        "- Answer:\n",
        "\n",
        "  Stanford NLP provides high-accuracy linguistic models based on deep learning and statistical parsing. It is used heavily in academia to benchmark syntactic and semantic understanding systems. In industry, Stanford NLP powers information extraction, document understanding, and question-answering systems. Its multilingual support and research-grade precision make it valuable where accuracy is more important than real-time speed.\n",
        "\n",
        "##Question 4: Describe the architecture and functioning of a Recurrent Natural Network (RNN).\n",
        "\n",
        "- Answer:\n",
        "\n",
        "  An RNN is a neural network designed for sequential data where past information needs to influence future predictions. It keeps a hidden state that stores memory from previous time steps and feeds it into the next step. This makes RNNs useful for tasks like language modeling and speech recognition. However, traditional RNNs suffer from vanishing gradients, making it difficult to learn long-term dependencies.\n",
        "\n",
        "##Question 5: What is the key difference between LSTM and GRU networks in NLP applications?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "  LSTM and GRU are both improved RNN architectures that address long-term dependency issues. LSTM has separate input, output, and forget gates to control memory flow, while GRU combines them into a single update and reset gate. GRU trains faster due to fewer parameters and works well for smaller datasets. LSTM may perform better when long-term memory retention is critical.\n",
        "\n",
        "##Question 6: Write a Python program using TextBlob to perform sentiment analysis on the following paragraph of text:\n",
        "\n",
        "  - ‚ÄúI had a great experience using the new mobile banking app. The interface is intuitive, and customer support was quick to resolve my issue. However, the app did crash once during a transaction, which was frustrating\"\n",
        "Your program should print out the polarity and subjectivity scores.\n",
        "(Include your Python code and output.)\n",
        "\n",
        "##Answer:\n",
        "‚úÖPython Code:\n",
        "\n"
      ],
      "metadata": {
        "id": "5bb86etljoJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TextBlob\n",
        "!pip install textblob\n",
        "\n",
        "# Download corpora for sentiment analysis\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSLkQBFj27YD",
        "outputId": "5efa158f-b8af-4dd1-d71b-0c5e0a627648"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-n1JhOFhyO6",
        "outputId": "5c477754-168e-4716-ca3b-74d5e7b73e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ],
      "source": [
        "# Perform Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = \"\"\"I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating\"\"\"\n",
        "\n",
        "analysis = TextBlob(text).sentiment\n",
        "\n",
        "print(\"Polarity:\", analysis.polarity)\n",
        "print(\"Subjectivity:\", analysis.subjectivity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7:\n",
        "\n",
        "Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "‚ÄúNatural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.‚Äù\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "###Answer:\n",
        "\n",
        "‚úÖPython Code:"
      ],
      "metadata": {
        "id": "KqcUOA-Y0Onm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # üî• required but not downloaded by default\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs382LUo37kp",
        "outputId": "85b001b4-4111-464b-a1f6-b942abb4cd58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # <-- Fixes the LookupError\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "paragraph = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand, interpret,\n",
        "and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation.\n",
        "As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(paragraph.lower())\n",
        "freq = FreqDist(tokens)\n",
        "print(freq.most_common(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zKNbsum0cBu",
        "outputId": "2d1aa661-e8ba-4d8f-d65b-875f7286816e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 7), ('.', 4), ('nlp', 3), ('and', 3), ('language', 2), ('is', 2), ('of', 2), ('natural', 1), ('processing', 1), ('(', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 8:\n",
        "\n",
        "Implement a basic LSTM model in Keras for a text classification task using the following dummy dataset. Your model should classify sentences as either positive (1) or negative (0).\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "\n",
        "‚ÄúI love this project‚Äù, #Positive\n",
        "\n",
        "‚ÄúThis is an amazing experience‚Äù, #Positive\n",
        "\n",
        "‚ÄúI hate waiting in line‚Äù, #Negative\n",
        "\n",
        "‚ÄúThis is the worst service‚Äù, #Negative\n",
        "\n",
        "‚ÄúAbsolutely fantastic!‚Äù #Positive\n",
        "\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "\n",
        "- Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on this data. You may use Keras with TensorFlow backend.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "###Answer:\n",
        "‚úÖPython Code:"
      ],
      "metadata": {
        "id": "bFIkgqEn0Wd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample Dataset\n",
        "texts = [\n",
        "    \"I love programming\",\n",
        "    \"Deep learning is amazing\",\n",
        "    \"NLP is a great field\",\n",
        "    \"I dislike bugs\",\n",
        "    \"Debugging can be frustrating\"\n",
        "]\n",
        "\n",
        "labels = np.array([1, 1, 1, 0, 0])   # üî• FIX ‚Äî convert to NumPy array\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "seq = tokenizer.texts_to_sequences(texts)\n",
        "padded = pad_sequences(seq, padding='post')\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and Train\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(padded, labels, epochs=20, verbose=0)\n",
        "\n",
        "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39llNW80VhL",
        "outputId": "67446bab-4d11-4b1c-ff43-5d1e965b4cf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.6000000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 9:\n",
        "\n",
        "Using spaCy, build a simple NLP pipeline that includes tokenization, lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "\n",
        "- ‚ÄúHomi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the development of India‚Äôs atomic energy program. He was the founding director of the Tata Institute of Fundamental Research (TIFR) and was instrumental in establishing the Atomic Energy Commission of India.‚Äù\n",
        "\n",
        "Write a Python program that processes this text using spaCy, then prints tokens, their lemmas, and any named entities found.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "###Answer:\n",
        "‚úÖPython Code:"
      ],
      "metadata": {
        "id": "6j_ytYX20aki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwoH1eez5j61",
        "outputId": "94e1e09f-3e77-46c2-af1c-03d994dba3bc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Runtime ‚Üí Restart runtime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "35NpnE9U57q8",
        "outputId": "956f5f27-54d6-4c6a-94bb-213e84a0138e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '‚Üí' (U+2192) (ipython-input-3605337568.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3605337568.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Runtime ‚Üí Restart runtime\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '‚Üí' (U+2192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Homi Jehangir Bhaba was a legend in Indian nuclear science.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"‚Üí\", token.lemma_)\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"|\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAmom5H80WQ_",
        "outputId": "327a3c10-a705-4068-c6fa-5c3ca2756905"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and Lemmas:\n",
            "Homi ‚Üí Homi\n",
            "Jehangir ‚Üí Jehangir\n",
            "Bhaba ‚Üí Bhaba\n",
            "was ‚Üí be\n",
            "a ‚Üí a\n",
            "legend ‚Üí legend\n",
            "in ‚Üí in\n",
            "Indian ‚Üí indian\n",
            "nuclear ‚Üí nuclear\n",
            "science ‚Üí science\n",
            ". ‚Üí .\n",
            "\n",
            "Named Entities:\n",
            "Homi Jehangir Bhaba | FAC\n",
            "Indian | NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 10:\n",
        "\n",
        "You are working on a chatbot for a mental health platform. Explain how you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "###Answer:\n",
        "  - A mental-health chatbot requires context understanding, emotion detection, and safe responses. I would clean text using spaCy (tokenization, lemmatization, entity recognition), convert words to embeddings, and feed them into an LSTM/GRU to classify intent and emotional tone. The model would generate predefined safe responses or escalate severe messages to human support. Ethical concerns include user privacy, avoiding harmful advice, and maintaining empathetic responses.\n",
        "  \n",
        "‚úÖPython Code:"
      ],
      "metadata": {
        "id": "kOjXVg3y0WB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# simplified dataset for demonstration\n",
        "X = np.array([\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1],\n",
        "    [0, 0]\n",
        "])  # features\n",
        "y = np.array([1, 0, 1, 0])  # target labels\n",
        "\n",
        "# corrected model\n",
        "model = Sequential([\n",
        "    Dense(4, activation='relu', input_shape=(2,)),   # üî• FIX: use input_shape instead of input_dim\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=25, verbose=0)\n",
        "\n",
        "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSwJHjHs07Oo",
        "outputId": "b54966e9-6453-45cb-9dd3-8643460ea874"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖPython Code:"
      ],
      "metadata": {
        "id": "4ar9BcXB02oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###************** END  **************"
      ],
      "metadata": {
        "id": "F_oL0bSxjto6"
      }
    }
  ]
}